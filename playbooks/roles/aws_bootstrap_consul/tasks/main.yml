---
# tasks file for aws-bootstrap
  - name: provision ec2 nodes
    action:
      module: ec2
      region: "{{ ec2_region }}"
      key_name: "{{ key_name }}"
      group: "{{ security_group }}"
      image: "{{ ec2_image }}"
      vpc_subnet_id: "{{ vpc_subnet_id }}"
      assign_public_ip: false
      instance_type: "{{ instance_type }}"
      count: "{{ instance_count }}"
      wait: yes
      wait_timeout: 180
      volumes:
      - device_name: /dev/sda1
        # confusingly, the root device is referred to as /dev/sda in AWS console, but
        # the device in linux is /dev/xvde. If you want to change the size of the underlying
        # volume as we are here, the device_name above MUST be /dev/sda. Unsure whether this
        # changes with different AMIs. Additional volumes do not seem to have this problem.
        volume_size: 24
        delete_on_termination: true
      instance_tags:                                                                                                                                                      
        Name: consul-cluster

    register: ec2_consul_cluster

  - name: write an inventory file 
    template: src=cluster.hosts.j2 dest='./consul.hosts' mode=0644 backup=yes

  - name: wait for the hosts to be available via ssh before moving on
    local_action: wait_for host='{{item.private_ip}}' port=22 delay=30 timeout=300 state=started
    with_items: ec2_consul_cluster.instances
    when: ec2_consul_cluster.instances is defined

  - name: add the new hosts to inventory
    add_host: name={{item.private_ip}} groups=instances ansible_ssh_user='{{ssh_user}}' public_ip='{{item.public_ip}}' private_ip='{{item.private_ip}}' public_dns_name='{{item.public_dns_name}}' private_dns_name='{{item.private_dns_name}}'
    with_items: ec2_consul_cluster.instances
    when: ec2_consul_cluster.instances is defined

  - name: save hosts file
    run_once: yes
    local_action: template src=hosts.j2 dest='./hosts_entry' mode=0644 backup=no
